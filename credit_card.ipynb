{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous exercises made you take a closer look at all the different parts of a neural network: \n",
    "* the architecture of a sequential Dense Neural Network, \n",
    "* the compilation method\n",
    "* the fitting.\n",
    "\n",
    "Let's now work on a real-life dataset that has **a lot of data**!\n",
    "\n",
    "**The dataset: `Credit Card Transactions`**\n",
    "\n",
    "For this open challenge, you will `work with data extracted from credit card transactions`. \n",
    "\n",
    "As this is `sensitive data`, only 3 columns are known out of a total 31: the rest have been transformed to `anonymize` them (in fact, they are `PCA projections of initial data`).\n",
    "\n",
    "The 3 known columns are:\n",
    "\n",
    "* `TIME`: the time elapsed between the transaction and the first transaction in the dataset\n",
    "* `AMOUNT`: the amount of the transaction\n",
    "* `CLASS` (our target): \n",
    "    * `0 : valid transaction` \n",
    "    * `1 : fraudulent transaction`\n",
    "\n",
    "‚ùì **Question** ‚ùì Start by downloading the dataset:\n",
    "* on the Kaggle website [here](https://www.kaggle.com/mlg-ulb/creditcardfraud) \n",
    "* or from our [URL](https://wagon-public-datasets.s3.amazonaws.com/Machine%20Learning%20Datasets/creditcard.csv) \n",
    "\n",
    "Load data to create `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4  0.502292  0.219422  0.215153   69.99      0  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "Shape of X: (284807, 30)\n",
      "Shape of y: (284807,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://wagon-public-datasets.s3.amazonaws.com/Machine%20Learning%20Datasets/creditcard.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "X = df.drop(\"Class\", axis=1)\n",
    "y = df[\"Class\"]\n",
    "\n",
    "print(\"X shape\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Rebalancing classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.998273\n",
       "1    0.001727\n",
       "Name: Class, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check class balance\n",
    "pd.Series(y).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è in this `fraud detection` challenge, **the classes are extremely imbalanced**:\n",
    "* 99.8 % of normal transactions\n",
    "* 0.2 % of fraudulent transactions\n",
    "\n",
    "**We won't be able to detect cases of fraud unless we apply some serious rebalancing strategies!**\n",
    "\n",
    "‚ùì **Question** ‚ùì\n",
    "1. **First**, create three separate splits `Train/Val/Test` from your dataset. It is extremely important to keep validation and testing sets **unbalanced** so that when you evaluate your model, it is done in true conditions, without data leakage. Keep your test set for the very last cell of this notebook!\n",
    "\n",
    "&nbsp;\n",
    "2. **Second**, rebalance your training set (and only this one). You have many choices:\n",
    "\n",
    "- Simply oversample the minority class randomly using plain Numpy functions (not the best option since you are duplicating rows and hence creating data leakage)\n",
    "- Or use <a href=\"https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/\">**`Synthetic Minority Oversampling Technique - SMOTE`**</a> to generate new datapoints by weighting the existing ones\n",
    "- In addition, you can also try a <a href=\"https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/\">**`RandomUnderSampler`**</a> to downsample the majority class a little bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set shape (170883, 30) (170883,)\n",
      "val set shape (56962, 30) (56962,)\n",
      "test set shape (56962, 30) (56962,)\n",
      "rebalanced train set shape (191911, 30) (191911,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val)\n",
    "\n",
    "print(\"train set shape\", X_train.shape, y_train.shape)\n",
    "print(\"val set shape\", X_val.shape, y_val.shape)\n",
    "print(\"test set shape\", X_test.shape, y_test.shape)\n",
    "\n",
    "smote = SMOTE(sampling_strategy=0.5, random_state=42)\n",
    "under_sampler = RandomUnderSampler(sampling_strategy=0.8, random_state=42)\n",
    "\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "X_train_resampled, y_train_resampled = under_sampler.fit_resample(X_train_resampled, y_train_resampled)\n",
    "\n",
    "print(\"rebalanced train set shape\", X_train_resampled.shape, y_train_resampled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Network iterations\n",
    "\n",
    "Now that you have rebalanced your classes, try to fit a neural network to optimize your test score. Feel free to use the following hints:\n",
    "\n",
    "- Normalize your inputs!\n",
    "    - Use preferably a [`Normalization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Normalization) layer inside the model to \"pipeline\" your preprocessing within your model. \n",
    "    - Or use sklearn's [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) outside of your model, applied your `X_train` and `X_val` and `X_test`.\n",
    "- Make your model overfit, then regularize  it using:\n",
    "    - Early Stopping criteria \n",
    "    - [`Dropout`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) layers\n",
    "    - or [`regularizers`](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers) layers\n",
    "- üö® Think carefully about the metrics you want to track and the loss function you want to use!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 12:00:26.840090: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999/2999 [==============================] - 3s 805us/step - loss: 0.1747 - accuracy: 0.9714 - auc: 0.9949 - val_loss: 0.0998 - val_accuracy: 0.9845 - val_auc: 0.9664\n",
      "Epoch 2/100\n",
      "2999/2999 [==============================] - 2s 698us/step - loss: 0.0957 - accuracy: 0.9786 - auc: 0.9971 - val_loss: 0.0529 - val_accuracy: 0.9962 - val_auc: 0.9653\n",
      "Epoch 3/100\n",
      "2999/2999 [==============================] - 2s 768us/step - loss: 0.0875 - accuracy: 0.9813 - auc: 0.9977 - val_loss: 0.0602 - val_accuracy: 0.9939 - val_auc: 0.9596\n",
      "Epoch 4/100\n",
      "2999/2999 [==============================] - 3s 837us/step - loss: 0.0827 - accuracy: 0.9824 - auc: 0.9980 - val_loss: 0.0664 - val_accuracy: 0.9916 - val_auc: 0.9540\n",
      "Epoch 5/100\n",
      "2999/2999 [==============================] - 2s 747us/step - loss: 0.0788 - accuracy: 0.9836 - auc: 0.9982 - val_loss: 0.0590 - val_accuracy: 0.9938 - val_auc: 0.9601\n",
      "Epoch 6/100\n",
      "2999/2999 [==============================] - 2s 777us/step - loss: 0.0756 - accuracy: 0.9843 - auc: 0.9984 - val_loss: 0.0654 - val_accuracy: 0.9909 - val_auc: 0.9584\n",
      "Epoch 7/100\n",
      "2999/2999 [==============================] - 2s 718us/step - loss: 0.0743 - accuracy: 0.9847 - auc: 0.9984 - val_loss: 0.0475 - val_accuracy: 0.9964 - val_auc: 0.9579\n",
      "Epoch 8/100\n",
      "2999/2999 [==============================] - 2s 821us/step - loss: 0.0723 - accuracy: 0.9851 - auc: 0.9985 - val_loss: 0.0461 - val_accuracy: 0.9966 - val_auc: 0.9542\n",
      "Epoch 9/100\n",
      "2999/2999 [==============================] - 2s 780us/step - loss: 0.0709 - accuracy: 0.9857 - auc: 0.9986 - val_loss: 0.0476 - val_accuracy: 0.9959 - val_auc: 0.9479\n",
      "Epoch 10/100\n",
      "2999/2999 [==============================] - 2s 755us/step - loss: 0.0705 - accuracy: 0.9859 - auc: 0.9985 - val_loss: 0.0636 - val_accuracy: 0.9903 - val_auc: 0.9533\n",
      "Epoch 11/100\n",
      "2999/2999 [==============================] - 2s 747us/step - loss: 0.0684 - accuracy: 0.9864 - auc: 0.9987 - val_loss: 0.0585 - val_accuracy: 0.9912 - val_auc: 0.9448\n",
      "Epoch 12/100\n",
      "2999/2999 [==============================] - 2s 726us/step - loss: 0.0672 - accuracy: 0.9867 - auc: 0.9987 - val_loss: 0.0413 - val_accuracy: 0.9965 - val_auc: 0.9331\n",
      "Epoch 13/100\n",
      "2999/2999 [==============================] - 2s 714us/step - loss: 0.0668 - accuracy: 0.9869 - auc: 0.9986 - val_loss: 0.0438 - val_accuracy: 0.9953 - val_auc: 0.9409\n",
      "Epoch 14/100\n",
      "2999/2999 [==============================] - 2s 717us/step - loss: 0.0665 - accuracy: 0.9868 - auc: 0.9987 - val_loss: 0.0495 - val_accuracy: 0.9935 - val_auc: 0.9425\n",
      "Epoch 15/100\n",
      "2999/2999 [==============================] - 2s 711us/step - loss: 0.0673 - accuracy: 0.9865 - auc: 0.9986 - val_loss: 0.0485 - val_accuracy: 0.9944 - val_auc: 0.9572\n",
      "Epoch 16/100\n",
      "2999/2999 [==============================] - 2s 688us/step - loss: 0.0659 - accuracy: 0.9866 - auc: 0.9987 - val_loss: 0.0422 - val_accuracy: 0.9962 - val_auc: 0.9458\n",
      "Epoch 17/100\n",
      "2999/2999 [==============================] - 2s 698us/step - loss: 0.0660 - accuracy: 0.9868 - auc: 0.9986 - val_loss: 0.0443 - val_accuracy: 0.9953 - val_auc: 0.9489\n",
      "Epoch 18/100\n",
      "2999/2999 [==============================] - 2s 688us/step - loss: 0.0654 - accuracy: 0.9866 - auc: 0.9987 - val_loss: 0.0437 - val_accuracy: 0.9952 - val_auc: 0.9378\n",
      "Epoch 19/100\n",
      "2999/2999 [==============================] - 2s 685us/step - loss: 0.0650 - accuracy: 0.9869 - auc: 0.9987 - val_loss: 0.0472 - val_accuracy: 0.9942 - val_auc: 0.9537\n",
      "Epoch 20/100\n",
      "2999/2999 [==============================] - 2s 677us/step - loss: 0.0644 - accuracy: 0.9872 - auc: 0.9987 - val_loss: 0.0398 - val_accuracy: 0.9970 - val_auc: 0.9400\n",
      "Epoch 21/100\n",
      "2999/2999 [==============================] - 2s 684us/step - loss: 0.0648 - accuracy: 0.9868 - auc: 0.9987 - val_loss: 0.0486 - val_accuracy: 0.9931 - val_auc: 0.9526\n",
      "Epoch 22/100\n",
      "2999/2999 [==============================] - 2s 674us/step - loss: 0.0646 - accuracy: 0.9873 - auc: 0.9987 - val_loss: 0.0478 - val_accuracy: 0.9944 - val_auc: 0.9519\n",
      "Epoch 23/100\n",
      "2999/2999 [==============================] - 2s 710us/step - loss: 0.0637 - accuracy: 0.9872 - auc: 0.9987 - val_loss: 0.0523 - val_accuracy: 0.9928 - val_auc: 0.9510\n",
      "Epoch 24/100\n",
      "2999/2999 [==============================] - 2s 682us/step - loss: 0.0634 - accuracy: 0.9874 - auc: 0.9987 - val_loss: 0.0517 - val_accuracy: 0.9936 - val_auc: 0.9529\n",
      "Epoch 25/100\n",
      "2999/2999 [==============================] - 2s 683us/step - loss: 0.0630 - accuracy: 0.9874 - auc: 0.9987 - val_loss: 0.0486 - val_accuracy: 0.9933 - val_auc: 0.9380\n",
      "Epoch 26/100\n",
      "2999/2999 [==============================] - 2s 684us/step - loss: 0.0637 - accuracy: 0.9872 - auc: 0.9986 - val_loss: 0.0602 - val_accuracy: 0.9894 - val_auc: 0.9578\n",
      "Epoch 27/100\n",
      "2999/2999 [==============================] - 2s 675us/step - loss: 0.0631 - accuracy: 0.9875 - auc: 0.9987 - val_loss: 0.0374 - val_accuracy: 0.9969 - val_auc: 0.9329\n",
      "Epoch 28/100\n",
      "2999/2999 [==============================] - 2s 683us/step - loss: 0.0633 - accuracy: 0.9871 - auc: 0.9988 - val_loss: 0.0450 - val_accuracy: 0.9938 - val_auc: 0.9426\n",
      "Epoch 29/100\n",
      "2999/2999 [==============================] - 2s 674us/step - loss: 0.0638 - accuracy: 0.9872 - auc: 0.9987 - val_loss: 0.0433 - val_accuracy: 0.9952 - val_auc: 0.9425\n",
      "Epoch 30/100\n",
      "2999/2999 [==============================] - 2s 682us/step - loss: 0.0627 - accuracy: 0.9874 - auc: 0.9987 - val_loss: 0.0483 - val_accuracy: 0.9940 - val_auc: 0.9453\n",
      "Epoch 31/100\n",
      "2999/2999 [==============================] - 2s 682us/step - loss: 0.0618 - accuracy: 0.9880 - auc: 0.9988 - val_loss: 0.0428 - val_accuracy: 0.9959 - val_auc: 0.9434\n",
      "Epoch 32/100\n",
      "2999/2999 [==============================] - 2s 674us/step - loss: 0.0626 - accuracy: 0.9875 - auc: 0.9987 - val_loss: 0.0400 - val_accuracy: 0.9964 - val_auc: 0.9381\n",
      "Epoch 33/100\n",
      "2999/2999 [==============================] - 2s 683us/step - loss: 0.0626 - accuracy: 0.9874 - auc: 0.9987 - val_loss: 0.0375 - val_accuracy: 0.9973 - val_auc: 0.9260\n",
      "Epoch 34/100\n",
      "2999/2999 [==============================] - 2s 681us/step - loss: 0.0630 - accuracy: 0.9871 - auc: 0.9986 - val_loss: 0.0505 - val_accuracy: 0.9926 - val_auc: 0.9447\n",
      "Epoch 35/100\n",
      "2999/2999 [==============================] - 2s 677us/step - loss: 0.0625 - accuracy: 0.9876 - auc: 0.9987 - val_loss: 0.0395 - val_accuracy: 0.9965 - val_auc: 0.9471\n",
      "Epoch 36/100\n",
      "2999/2999 [==============================] - 2s 682us/step - loss: 0.0625 - accuracy: 0.9873 - auc: 0.9986 - val_loss: 0.0516 - val_accuracy: 0.9923 - val_auc: 0.9485\n",
      "Epoch 37/100\n",
      "2999/2999 [==============================] - 2s 691us/step - loss: 0.0626 - accuracy: 0.9872 - auc: 0.9986 - val_loss: 0.0425 - val_accuracy: 0.9953 - val_auc: 0.9440\n",
      "1781/1781 [==============================] - 1s 361us/step - loss: 0.0382 - accuracy: 0.9967 - auc: 0.9643\n",
      "test loss 0.0382\n",
      "test acc 99.67%\n",
      "test AUC 0.9643\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.InputLayer(input_shape=(X_train_scaled.shape[1],)),\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', keras.metrics.AUC(name='auc')])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_resampled,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "test_loss, test_accuracy, test_auc = model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "print(f\"test loss {test_loss:.4f}\")\n",
    "print(f\"test acc {test_accuracy*100:.2f}%\")\n",
    "print(f\"test AUC {test_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Test your score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store below your real test performance on a (`X_test`, `y_test`) representative sample of the original unbalanced dataset into `precision` and `recall` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1781/1781 [==============================] - 1s 300us/step\n",
      "prec 0.3271\n",
      "recall 0.8878\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "\n",
    "print(f\"prec {precision:.4f}\")\n",
    "print(f\"recall {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.6, pytest-7.1.3, pluggy-1.0.0 -- /Users/reecepalmer/.pyenv/versions/3.10.6/envs/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/reecepalmer/Code/RPalmr/06-Deep-Learning/02-Optimizer-loss-and-fitting/data-credit-card-challenge/tests\n",
      "plugins: asyncio-0.19.0, dash-2.14.0, typeguard-2.13.3, anyio-3.6.2, hydra-core-1.3.2\n",
      "asyncio: mode=strict\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "test_solution.py::TestSolution::test_is_score_good_enough \u001b[32mPASSED\u001b[0m\u001b[32m         [ 50%]\u001b[0m\n",
      "test_solution.py::TestSolution::test_is_test_set_representative \u001b[32mPASSED\u001b[0m\u001b[32m   [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.08s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "üíØ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/solution.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed solution step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('solution',\n",
    "    precision=precision,\n",
    "    recall=recall,\n",
    "    fraud_number=len(y_test[y_test == 1]),\n",
    "    non_fraud_number=len(y_test[y_test == 0]),\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÅ Optional: Read Google's solution for this challenge\n",
    "Congratulations for finishing all challenges for this session!\n",
    "\n",
    "To conclude, take some time to read Google's own solution direcly [on Colab here](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/structured_data/imbalanced_data.ipynb). \n",
    "\n",
    "You will discover interesting techniques and best practices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
